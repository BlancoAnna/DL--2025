{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a8770448",
      "metadata": {
        "id": "a8770448"
      },
      "source": [
        "### Fundamentals of Natural Language Processing\n",
        "# Negation and Uncertainty Detection using a Machine-Learning Based Approach\n",
        "\n",
        "*Authors:*\n",
        "\n",
        "> *Anna Blanco, Agustina Lazzati, Stanislav Bultaskii and Queralt Salvadó*\n",
        "\n",
        "*Aims:*\n",
        "> In natural language processing, accurately identifying negation and uncertainty is crucial for tasks like clinical information extraction or sentiment analysis. This project implements a deep learning-based approach to automatically detect such linguistic phenomena in text. By training sequence labeling models on annotated data, the system learns to recognize cue words (e.g., \"not\", \"unlikely\") and their corresponding scopes—the parts of the sentence affected by them. The goal is to improve text understanding by distinguishing between negated and uncertain statements."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n"
      ],
      "metadata": {
        "id": "AApkfTGqvn5w"
      },
      "id": "AApkfTGqvn5w"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "OD99InlolDaK",
      "metadata": {
        "id": "OD99InlolDaK"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries and functions\n",
        "import json\n",
        "import spacy\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to load preprocessed data from a .pkl file containing training and test data for the negation and uncertainty cues and scopes:\n",
        "\n"
      ],
      "metadata": {
        "id": "zz21Q63bxHZf"
      },
      "id": "zz21Q63bxHZf"
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"lstm_data.pkl\", \"rb\") as f:\n",
        "    data_dict = pickle.load(f)\n",
        "\n",
        "lstm_train_data_neg_cue = data_dict[\"lstm_train_data_neg_cue\"]\n",
        "lstm_train_data_neg_scope = data_dict[\"lstm_train_data_neg_scope\"]\n",
        "lstm_train_data_unc_cue = data_dict[\"lstm_train_data_unc_cue\"]\n",
        "lstm_train_data_unc_scope = data_dict[\"lstm_train_data_unc_scope\"]\n",
        "\n",
        "lstm_test_data_neg_cue = data_dict[\"lstm_test_data_neg_cue\"]\n",
        "lstm_test_data_neg_scope = data_dict[\"lstm_test_data_neg_scope\"]\n",
        "lstm_test_data_unc_cue = data_dict[\"lstm_test_data_unc_cue\"]\n",
        "lstm_test_data_unc_scope = data_dict[\"lstm_test_data_unc_scope\"]\n",
        "\n",
        "print(lstm_train_data_neg_cue[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpRP1CIuFwHz",
        "outputId": "341a5440-4d87-489b-c0e2-cd3a320c1cb0"
      },
      "id": "YpRP1CIuFwHz",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['antecedents', 'alergia', 'a', 'penicilina', 'y', 'cloramfenicol', '.'], [0, 0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create unified labels by combining cue and scope labels into a single sequence:\n",
        "\n",
        "\"NEG_1\" or \"NSCO_1\" for negation\n",
        "\n",
        "\"UNC_1\" or \"UNSCO_1\" for uncertainty"
      ],
      "metadata": {
        "id": "LiouQLmjxcUe"
      },
      "id": "LiouQLmjxcUe"
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_labels(cue_labels, scope_labels, cue_prefix=\"CUE\", scope_prefix=\"SCOPE\"):\n",
        "    merged = []\n",
        "    for cue, scope in zip(cue_labels, scope_labels):\n",
        "        if cue != 0:\n",
        "            merged.append(f\"{cue_prefix}_{str(cue)}\")\n",
        "        elif scope != 0:\n",
        "            merged.append(f\"{scope_prefix}_{str(scope)}\")\n",
        "        else:\n",
        "            merged.append(\"0\")\n",
        "    return merged"
      ],
      "metadata": {
        "id": "pczB1GJeHk5G"
      },
      "id": "pczB1GJeHk5G",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate datasets of (tokens, merged_labels) for negation and uncertainty:"
      ],
      "metadata": {
        "id": "TbaYaMZixiFS"
      },
      "id": "TbaYaMZixiFS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge negation data\n",
        "lstm_train_data_neg = [\n",
        "    (tokens, merge_labels(cue_labels, scope_labels, cue_prefix=\"NEG\", scope_prefix=\"NSCO\"))\n",
        "    for (tokens, cue_labels), (_, scope_labels) in zip(lstm_train_data_neg_cue, lstm_train_data_neg_scope)\n",
        "]\n",
        "\n",
        "lstm_test_data_neg = [\n",
        "    (tokens, merge_labels(cue_labels, scope_labels, cue_prefix=\"NEG\", scope_prefix=\"NSCO\"))\n",
        "    for (tokens, cue_labels), (_, scope_labels) in zip(lstm_test_data_neg_cue, lstm_test_data_neg_scope)\n",
        "]\n",
        "\n",
        "# Similarly for uncertainty\n",
        "lstm_train_data_unc = [\n",
        "    (tokens, merge_labels(cue_labels, scope_labels, cue_prefix=\"UNC\", scope_prefix=\"UNSCO\"))\n",
        "    for (tokens, cue_labels), (_, scope_labels) in zip(lstm_train_data_unc_cue, lstm_train_data_unc_scope)\n",
        "]\n",
        "\n",
        "lstm_test_data_unc = [\n",
        "    (tokens, merge_labels(cue_labels, scope_labels, cue_prefix=\"UNC\", scope_prefix=\"UNSCO\"))\n",
        "    for (tokens, cue_labels), (_, scope_labels) in zip(lstm_test_data_unc_cue, lstm_test_data_unc_scope)\n",
        "]\n",
        "\n",
        "print(lstm_train_data_neg[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Cy16eUrHl38",
        "outputId": "856a7341-f89e-4bd3-add7-fde0fab5d1b0"
      },
      "id": "4Cy16eUrHl38",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['no', 'habitos', 'toxicos', '.'], ['NEG_1', 'NSCO_1', 'NSCO_1', 'NSCO_1'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext\n",
        "\n",
        "import fasttext\n",
        "\n",
        "# Download the English fastText model\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "# Unzip the downloaded file\n",
        "!gunzip cc.en.300.bin.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBh1-_eBGo0J",
        "outputId": "d21cf678-e8b2-4ae8-e30d-701d6f79902c"
      },
      "id": "LBh1-_eBGo0J",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (2.0.2)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4313489 sha256=a36657305a0ba55f6c07047bd7564ada5919e796ea2d5b8ef8e2165d1a59436b\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n",
            "--2025-05-27 17:18:30--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.108, 3.163.189.14, 3.163.189.51, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4503593528 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘cc.en.300.bin.gz’\n",
            "\n",
            "cc.en.300.bin.gz    100%[===================>]   4.19G   152MB/s    in 27s     \n",
            "\n",
            "2025-05-27 17:18:57 (161 MB/s) - ‘cc.en.300.bin.gz’ saved [4503593528/4503593528]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained FastText model (English, 300-dimensional vectors)\n",
        "fasttext_model = fasttext.load_model(\"cc.en.300.bin\")"
      ],
      "metadata": {
        "id": "tySN5ggvGv75"
      },
      "id": "tySN5ggvGv75",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create:\n",
        "\n",
        "word2idx: maps each unique word to an index.\n",
        "\n",
        "label2idx: maps each label (e.g., \"NEG_1\") to an index."
      ],
      "metadata": {
        "id": "q5Zj2Wc6xotq"
      },
      "id": "q5Zj2Wc6xotq"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    word2idx = defaultdict(lambda: 0)  # unknown token index = 0\n",
        "    idx = 1\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            if word not in word2idx:\n",
        "                word2idx[word] = idx\n",
        "                idx += 1\n",
        "    return dict(word2idx)\n",
        "\n",
        "def build_label_vocab(labels_list):\n",
        "    label_set = set()\n",
        "    for labels in labels_list:\n",
        "        label_set.update(labels)\n",
        "    label2idx = {label: i for i, label in enumerate(sorted(label_set))}\n",
        "    return label2idx\n",
        "\n",
        "# Train and Test data for negations\n",
        "all_train_sentences_neg = [x[0] for x in lstm_train_data_neg] # list of token lists\n",
        "all_train_labels_neg = [x[1] for x in lstm_train_data_neg] # list of label lists\n",
        "\n",
        "all_test_sentences_neg = [x[0] for x in lstm_test_data_neg]\n",
        "all_test_labels_neg = [x[1] for x in lstm_test_data_neg]\n",
        "\n",
        "# Train and Test data for uncertainties\n",
        "all_train_sentences_unc = [x[0] for x in lstm_train_data_unc]\n",
        "all_train_labels_unc = [x[1] for x in lstm_train_data_unc]\n",
        "\n",
        "all_test_sentences_unc = [x[0] for x in lstm_test_data_unc]\n",
        "all_test_labels_unc = [x[1] for x in lstm_test_data_unc]\n",
        "\n",
        "# Merge all sentences and labels into single lists\n",
        "all_sentences = (\n",
        "    all_train_sentences_neg + all_test_sentences_neg +\n",
        "    all_train_sentences_unc + all_test_sentences_unc\n",
        ")\n",
        "\n",
        "all_labels = (\n",
        "    all_train_labels_neg + all_test_labels_neg +\n",
        "    all_train_labels_unc + all_test_labels_unc\n",
        ")\n",
        "\n",
        "# Build vocabularies\n",
        "word2idx = build_vocab(all_sentences)\n",
        "label2idx = build_label_vocab(all_labels)\n",
        "\n",
        "print(f\"Vocabulary size (words): {len(word2idx)}\")\n",
        "print(f\"Number of unique labels: {len(label2idx)}\")\n",
        "\n",
        "# Optional: check example mappings\n",
        "print(f\"Example word2idx: {list(word2idx.items())[:10]}\")\n",
        "print(f\"Example label2idx: {list(label2idx.items())[:10]}\")"
      ],
      "metadata": {
        "id": "IplmnSStKkds",
        "outputId": "9d600ce0-de65-437b-dd37-7cea88c0725c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "IplmnSStKkds",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size (words): 23359\n",
            "Number of unique labels: 5\n",
            "Example word2idx: [(' ', 1), ('nº', 2), ('historia', 3), ('clinica', 4), (':', 5), ('*', 6), ('nºepisodi', 7), ('sexe', 8), ('home', 9), ('data', 10)]\n",
            "Example label2idx: [('0', 0), ('NEG_1', 1), ('NSCO_1', 2), ('UNC_1', 3), ('UNSCO_1', 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\"\"\"\n",
        "Initializes an embedding matrix using FastText. If a word is not found, it is assigned a random vector.\n",
        "\n",
        "\"\"\"\n",
        "vocab_size = len(word2idx) + 1  # +1 for padding idx=0\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "\n",
        "for word, idx in word2idx.items():\n",
        "    try:\n",
        "        embedding_vector = fasttext_model.get_word_vector(word)\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "    except KeyError:\n",
        "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(300,))"
      ],
      "metadata": {
        "id": "u4rGVxVcKuQg"
      },
      "id": "u4rGVxVcKuQg",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def encode_sentences(sentences, word2idx):\n",
        "    encoded = []\n",
        "    for sent in sentences:\n",
        "        encoded.append([word2idx.get(word, 0) for word in sent])\n",
        "    return encoded\n",
        "\n",
        "def encode_labels(labels, label2idx):\n",
        "    encoded = []\n",
        "    for lab_seq in labels:\n",
        "        encoded.append([label2idx[str(label)] for label in lab_seq])\n",
        "    return encoded\n",
        "\n",
        "# Negation data\n",
        "X_train_neg = encode_sentences(all_train_sentences_neg, word2idx)\n",
        "y_train_neg = encode_labels(all_train_labels_neg, label2idx)\n",
        "\n",
        "X_test_neg = encode_sentences(all_test_sentences_neg, word2idx)\n",
        "y_test_neg = encode_labels(all_test_labels_neg, label2idx)\n",
        "\n",
        "# Uncertainty data\n",
        "X_train_unc = encode_sentences(all_train_sentences_unc, word2idx)\n",
        "y_train_unc = encode_labels(all_train_labels_unc, label2idx)\n",
        "\n",
        "X_test_unc = encode_sentences(all_test_sentences_unc, word2idx)\n",
        "y_test_unc = encode_labels(all_test_labels_unc, label2idx)\n",
        "\n",
        "# Show an example\n",
        "print(X_train_neg[2], y_train_neg[2])"
      ],
      "metadata": {
        "id": "rhzvfE6DOkx5",
        "outputId": "e777d9f8-4d9f-417f-f5e5-8b25f1299eec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "rhzvfE6DOkx5",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[45, 46, 47, 48, 49, 50, 44] [0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\"\"\"\n",
        "Pads sequences so all have the same length, which is necessary for batch training.\n",
        "\"\"\"\n",
        "def pad_sequences(sequences, pad_value=0):\n",
        "    # Convert lists of indices to torch tensors\n",
        "    tensor_seqs = [torch.tensor(seq) for seq in sequences]\n",
        "    # Pad sequences to the max length in the batch\n",
        "    padded_seqs = pad_sequence(tensor_seqs, batch_first=True, padding_value=pad_value)\n",
        "    return padded_seqs\n",
        "\n",
        "# Pad inputs and labels (negation)\n",
        "X_train_neg_padded = pad_sequences(X_train_neg, pad_value=0)\n",
        "y_train_neg_padded = pad_sequences(y_train_neg, pad_value=label2idx.get('0', 0))\n",
        "\n",
        "X_test_neg_padded = pad_sequences(X_test_neg, pad_value=0)\n",
        "y_test_neg_padded = pad_sequences(y_test_neg, pad_value=label2idx.get('0', 0))\n",
        "\n",
        "# Pad inputs and labels (uncertainty)\n",
        "X_train_unc_padded = pad_sequences(X_train_unc, pad_value=0)\n",
        "y_train_unc_padded = pad_sequences(y_train_unc, pad_value=label2idx.get('0', 0))\n",
        "\n",
        "X_test_unc_padded = pad_sequences(X_test_unc, pad_value=0)\n",
        "y_test_unc_padded = pad_sequences(y_test_unc, pad_value=label2idx.get('0', 0))"
      ],
      "metadata": {
        "id": "Z1qT8Ql5QIwh"
      },
      "id": "Z1qT8Ql5QIwh",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\"\"\"\n",
        "Wraps data into PyTorch Dataset and DataLoader objects for use in model training and evaluation.\n",
        "\"\"\"\n",
        "class SequenceTaggingDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Create dataset objects\n",
        "train_dataset_neg = SequenceTaggingDataset(X_train_neg_padded, y_train_neg_padded)\n",
        "test_dataset_neg = SequenceTaggingDataset(X_test_neg_padded, y_test_neg_padded)\n",
        "\n",
        "train_dataset_unc = SequenceTaggingDataset(X_train_unc_padded, y_train_unc_padded)\n",
        "test_dataset_unc = SequenceTaggingDataset(X_test_unc_padded, y_test_unc_padded)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader_neg = DataLoader(train_dataset_neg, batch_size=32, shuffle=True)\n",
        "test_loader_neg = DataLoader(test_dataset_neg, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_unc = DataLoader(train_dataset_unc, batch_size=32, shuffle=True)\n",
        "test_loader_unc = DataLoader(test_dataset_unc, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "yKd8eUM2P_jM"
      },
      "id": "yKd8eUM2P_jM",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Model"
      ],
      "metadata": {
        "id": "0-hz5_jlvjrZ"
      },
      "id": "0-hz5_jlvjrZ"
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, output_dim):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        vocab_size, embedding_dim = embedding_matrix.shape\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        output = self.fc(lstm_out)\n",
        "        return output"
      ],
      "metadata": {
        "id": "h7F6vrodvzEi"
      },
      "id": "h7F6vrodvzEi",
      "execution_count": 14,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}