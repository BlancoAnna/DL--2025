{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a8770448",
      "metadata": {
        "id": "a8770448"
      },
      "source": [
        "### Fundamentals of Natural Language Processing\n",
        "# Negation and Uncertainty Detection using a Machine-Learning Based Approach\n",
        "\n",
        "*Authors:*\n",
        "\n",
        "> *Anna Blanco, Agustina Lazzati, Stanislav Bultaskii, Queralt Salvadó*\n",
        "\n",
        "*Aims:*\n",
        "> Rewrite for DL"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "_scE_qkujYd3"
      },
      "id": "_scE_qkujYd3"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "OD99InlolDaK",
      "metadata": {
        "id": "OD99InlolDaK"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries and functions\n",
        "import json\n",
        "import spacy\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"lstm_data.pkl\", \"rb\") as f:\n",
        "    data_dict = pickle.load(f)\n",
        "\n",
        "lstm_train_data_neg_cue = data_dict[\"lstm_train_data_neg_cue\"]\n",
        "lstm_train_data_neg_scope = data_dict[\"lstm_train_data_neg_scope\"]\n",
        "lstm_train_data_unc_cue = data_dict[\"lstm_train_data_unc_cue\"]\n",
        "lstm_train_data_unc_scope = data_dict[\"lstm_train_data_unc_scope\"]\n",
        "\n",
        "lstm_test_data_neg_cue = data_dict[\"lstm_test_data_neg_cue\"]\n",
        "lstm_test_data_neg_scope = data_dict[\"lstm_test_data_neg_scope\"]\n",
        "lstm_test_data_unc_cue = data_dict[\"lstm_test_data_unc_cue\"]\n",
        "lstm_test_data_unc_scope = data_dict[\"lstm_test_data_unc_scope\"]\n",
        "\n",
        "print(lstm_train_data_neg_cue[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpRP1CIuFwHz",
        "outputId": "b17876fd-44b8-4b88-c038-0d46c741a94d"
      },
      "id": "YpRP1CIuFwHz",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['antecedents', 'alergia', 'a', 'penicilina', 'y', 'cloramfenicol', '.'], [0, 0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_labels(cue_labels, scope_labels, cue_prefix=\"CUE\", scope_prefix=\"SCOPE\"):\n",
        "    merged = []\n",
        "    for cue, scope in zip(cue_labels, scope_labels):\n",
        "        if cue != 0:\n",
        "            merged.append(f\"{cue_prefix}_{str(cue)}\")\n",
        "        elif scope != 0:\n",
        "            merged.append(f\"{scope_prefix}_{str(scope)}\")\n",
        "        else:\n",
        "            merged.append(\"0\")\n",
        "    return merged"
      ],
      "metadata": {
        "id": "pczB1GJeHk5G"
      },
      "id": "pczB1GJeHk5G",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge negation data\n",
        "lstm_train_data_neg = [\n",
        "    (tokens, merge_labels(cue_labels, scope_labels, cue_prefix=\"NEG\", scope_prefix=\"NSCO\"))\n",
        "    for (tokens, cue_labels), (_, scope_labels) in zip(lstm_train_data_neg_cue, lstm_train_data_neg_scope)\n",
        "]\n",
        "\n",
        "lstm_test_data_neg = [\n",
        "    (tokens, merge_labels(cue_labels, scope_labels, cue_prefix=\"NEG\", scope_prefix=\"NSCO\"))\n",
        "    for (tokens, cue_labels), (_, scope_labels) in zip(lstm_test_data_neg_cue, lstm_test_data_neg_scope)\n",
        "]\n",
        "\n",
        "# Similarly for uncertainty\n",
        "lstm_train_data_unc = [\n",
        "    (tokens, merge_labels(cue_labels, scope_labels, cue_prefix=\"UNC\", scope_prefix=\"UNSCO\"))\n",
        "    for (tokens, cue_labels), (_, scope_labels) in zip(lstm_train_data_unc_cue, lstm_train_data_unc_scope)\n",
        "]\n",
        "\n",
        "lstm_test_data_unc = [\n",
        "    (tokens, merge_labels(cue_labels, scope_labels, cue_prefix=\"UNC\", scope_prefix=\"UNSCO\"))\n",
        "    for (tokens, cue_labels), (_, scope_labels) in zip(lstm_test_data_unc_cue, lstm_test_data_unc_scope)\n",
        "]\n",
        "\n",
        "print(lstm_train_data_neg[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Cy16eUrHl38",
        "outputId": "a535a8f8-e2f4-4adf-83c3-eef1dd214c1e"
      },
      "id": "4Cy16eUrHl38",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['no', 'habitos', 'toxicos', '.'], ['NEG_1', 'NSCO_1', 'NSCO_1', 'NSCO_1'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext\n",
        "\n",
        "import fasttext\n",
        "\n",
        "# Download the English fastText model\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "# Unzip the downloaded file\n",
        "!gunzip cc.en.300.bin.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBh1-_eBGo0J",
        "outputId": "8f326b31-1dcc-4d6b-eb60-6e2f47686c2b"
      },
      "id": "LBh1-_eBGo0J",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.11/dist-packages (0.9.3)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.11/dist-packages (from fasttext) (2.13.6)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (2.0.2)\n",
            "--2025-05-27 16:38:54--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.74.118, 13.227.74.45, 13.227.74.9, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.74.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4503593528 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘cc.en.300.bin.gz’\n",
            "\n",
            "cc.en.300.bin.gz     55%[==========>         ]   2.32G   237MB/s    eta 9s     "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained FastText model (English, 300-dimensional vectors)\n",
        "fasttext_model = fasttext.load_model(\"cc.en.300.bin\")"
      ],
      "metadata": {
        "id": "tySN5ggvGv75"
      },
      "id": "tySN5ggvGv75",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    word2idx = defaultdict(lambda: 0)  # unknown token index = 0\n",
        "    idx = 1\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            if word not in word2idx:\n",
        "                word2idx[word] = idx\n",
        "                idx += 1\n",
        "    return dict(word2idx)\n",
        "\n",
        "def build_label_vocab(labels_list):\n",
        "    label_set = set()\n",
        "    for labels in labels_list:\n",
        "        label_set.update(labels)\n",
        "    label2idx = {label: i for i, label in enumerate(sorted(label_set))}\n",
        "    return label2idx\n",
        "\n",
        "# Train and Test data for negations\n",
        "all_train_sentences_neg = [x[0] for x in lstm_train_data_neg] # list of token lists\n",
        "all_train_labels_neg = [x[1] for x in lstm_train_data_neg] # list of label lists\n",
        "\n",
        "all_test_sentences_neg = [x[0] for x in lstm_test_data_neg]\n",
        "all_test_labels_neg = [x[1] for x in lstm_test_data_neg]\n",
        "\n",
        "# Train and Test data for uncertainties\n",
        "all_train_sentences_unc = [x[0] for x in lstm_train_data_unc]\n",
        "all_train_labels_unc = [x[1] for x in lstm_train_data_unc]\n",
        "\n",
        "all_test_sentences_unc = [x[0] for x in lstm_test_data_unc]\n",
        "all_test_labels_unc = [x[1] for x in lstm_test_data_unc]\n",
        "\n",
        "# Merge all sentences and labels into single lists\n",
        "all_sentences = (\n",
        "    all_train_sentences_neg + all_test_sentences_neg +\n",
        "    all_train_sentences_unc + all_test_sentences_unc\n",
        ")\n",
        "\n",
        "all_labels = (\n",
        "    all_train_labels_neg + all_test_labels_neg +\n",
        "    all_train_labels_unc + all_test_labels_unc\n",
        ")\n",
        "\n",
        "# Build vocabularies\n",
        "word2idx = build_vocab(all_sentences)\n",
        "label2idx = build_label_vocab(all_labels)\n",
        "\n",
        "print(f\"Vocabulary size (words): {len(word2idx)}\")\n",
        "print(f\"Number of unique labels: {len(label2idx)}\")\n",
        "\n",
        "# Optional: check example mappings\n",
        "print(f\"Example word2idx: {list(word2idx.items())[:10]}\")\n",
        "print(f\"Example label2idx: {list(label2idx.items())[:10]}\")"
      ],
      "metadata": {
        "id": "IplmnSStKkds"
      },
      "id": "IplmnSStKkds",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "vocab_size = len(word2idx) + 1  # +1 for padding idx=0\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "\n",
        "for word, idx in word2idx.items():\n",
        "    try:\n",
        "        embedding_vector = fasttext_model.get_word_vector(word)\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "    except KeyError:\n",
        "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(300,))"
      ],
      "metadata": {
        "id": "u4rGVxVcKuQg"
      },
      "id": "u4rGVxVcKuQg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sentences(sentences, word2idx):\n",
        "    encoded = []\n",
        "    for sent in sentences:\n",
        "        encoded.append([word2idx.get(word, 0) for word in sent])\n",
        "    return encoded\n",
        "\n",
        "def encode_labels(labels, label2idx):\n",
        "    encoded = []\n",
        "    for lab_seq in labels:\n",
        "        encoded.append([label2idx[str(label)] for label in lab_seq])\n",
        "    return encoded\n",
        "\n",
        "# Negation data\n",
        "X_train_neg = encode_sentences(all_train_sentences_neg, word2idx)\n",
        "y_train_neg = encode_labels(all_train_labels_neg, label2idx)\n",
        "\n",
        "X_test_neg = encode_sentences(all_test_sentences_neg, word2idx)\n",
        "y_test_neg = encode_labels(all_test_labels_neg, label2idx)\n",
        "\n",
        "# Uncertainty data\n",
        "X_train_unc = encode_sentences(all_train_sentences_unc, word2idx)\n",
        "y_train_unc = encode_labels(all_train_labels_unc, label2idx)\n",
        "\n",
        "X_test_unc = encode_sentences(all_test_sentences_unc, word2idx)\n",
        "y_test_unc = encode_labels(all_test_labels_unc, label2idx)\n",
        "\n",
        "# Show an example\n",
        "print(X_train_neg[2], y_train_neg[2])"
      ],
      "metadata": {
        "id": "rhzvfE6DOkx5"
      },
      "id": "rhzvfE6DOkx5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "def pad_sequences(sequences, pad_value=0):\n",
        "    # Convert lists of indices to torch tensors\n",
        "    tensor_seqs = [torch.tensor(seq) for seq in sequences]\n",
        "    # Pad sequences to the max length in the batch\n",
        "    padded_seqs = pad_sequence(tensor_seqs, batch_first=True, padding_value=pad_value)\n",
        "    return padded_seqs\n",
        "\n",
        "# Pad inputs and labels (negation)\n",
        "X_train_neg_padded = pad_sequences(X_train_neg, pad_value=0)\n",
        "y_train_neg_padded = pad_sequences(y_train_neg, pad_value=label2idx.get('0', 0))\n",
        "\n",
        "X_test_neg_padded = pad_sequences(X_test_neg, pad_value=0)\n",
        "y_test_neg_padded = pad_sequences(y_test_neg, pad_value=label2idx.get('0', 0))\n",
        "\n",
        "# Pad inputs and labels (uncertainty)\n",
        "X_train_unc_padded = pad_sequences(X_train_unc, pad_value=0)\n",
        "y_train_unc_padded = pad_sequences(y_train_unc, pad_value=label2idx.get('0', 0))\n",
        "\n",
        "X_test_unc_padded = pad_sequences(X_test_unc, pad_value=0)\n",
        "y_test_unc_padded = pad_sequences(y_test_unc, pad_value=label2idx.get('0', 0))"
      ],
      "metadata": {
        "id": "Z1qT8Ql5QIwh"
      },
      "id": "Z1qT8Ql5QIwh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SequenceTaggingDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Create dataset objects\n",
        "train_dataset_neg = SequenceTaggingDataset(X_train_neg_padded, y_train_neg_padded)\n",
        "test_dataset_neg = SequenceTaggingDataset(X_test_neg_padded, y_test_neg_padded)\n",
        "\n",
        "train_dataset_unc = SequenceTaggingDataset(X_train_unc_padded, y_train_unc_padded)\n",
        "test_dataset_unc = SequenceTaggingDataset(X_test_unc_padded, y_test_unc_padded)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader_neg = DataLoader(train_dataset_neg, batch_size=32, shuffle=True)\n",
        "test_loader_neg = DataLoader(test_dataset_neg, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_unc = DataLoader(train_dataset_unc, batch_size=32, shuffle=True)\n",
        "test_loader_unc = DataLoader(test_dataset_unc, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "yKd8eUM2P_jM"
      },
      "id": "yKd8eUM2P_jM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Model"
      ],
      "metadata": {
        "id": "8ek0CVr2jb-m"
      },
      "id": "8ek0CVr2jb-m"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, output_dim):\n",
        "        super(LSTM, self).__init__()\n",
        "        vocab_size, embedding_dim = embedding_matrix.shape\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        output = self.fc(lstm_out)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "LUMD80IgjeMN"
      },
      "id": "LUMD80IgjeMN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 128\n",
        "output_dim = len(label2idx)\n",
        "model = LSTM(embedding_matrix, hidden_dim, output_dim)\n"
      ],
      "metadata": {
        "id": "6HTHfkqCj6Bl"
      },
      "id": "6HTHfkqCj6Bl",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}